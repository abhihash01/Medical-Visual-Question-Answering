#!/bin/bash
#SBATCH --job-name=data_rad.job
#SBATCH --output=/home/aabhilash/med_vlms/llava-med-1/LLaVAMed100/running_files/data_rad_evaluation_out.txt
#SBATCH --error=/home/aabhilash/med_vlms/llava-med-1/LLaVAMed100/running_files/data_rad_evaluation_err.txt
#SBATCH --time=2-00:00
#SBATCH --mem=50000
#SBATCH --gres=gpu:1

# Test CUDA compiler (not needed by deep learning people, we just use the python libraries)
#/cm/shared/apps/cuda11.1/toolkit/11.1.1/bin/nvcc -o saxpy /home/<netid>/cuda_c_code/saxpy.cu && ./saxpy

# Test nvidia-smi
nvidia-smi

echo "Running job in directory: $(pwd)"

source /home/aabhilash/miniconda3/etc/profile.d/conda.sh
# Test Python conda environment
conda activate llava-med-1 
/home/aabhilash/miniconda3/envs/llava-med-1/bin/python -m llava.model.apply_delta \
    --base /home/aabhilash/med_vlms/llama_weigths/llama-7b \
    --target /home/aabhilash/med_vlms/llava-med-1/merged_weights/data_rad_merged_model \
    --delta /home/aabhilash/med_vlms/llava-med-1/fine_tuned_delta_weights/data_RAD-9epoch_delta